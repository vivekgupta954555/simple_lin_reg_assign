{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cea6b20b-717a-482d-a42a-42432f17e94f",
   "metadata": {},
   "source": [
    "## QUES NO 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dedb6a0-7f09-44f5-bc4d-80d615480691",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple linear regression has only one x and one y variable. Multiple linear regression has one y and \n",
    "two or more x variables. For instance, when we predict rent based on square feet alone that is simple\n",
    "linear regression.\n",
    "\n",
    "Simple linear regression \n",
    "Simple linear regression is a statistical method for establishing the relationship between two variables\n",
    "using a straight line. The line is drawn by finding the slope and intercept, which define the line and\n",
    "minimize regression errors\n",
    "\n",
    "Example:\n",
    "    We could use to predict weight if we knew an individual's height ( by giving some number amount of the \n",
    "    data like weight and height as a training dataset for our  model\n",
    "    \n",
    "Multiple linear regression  \n",
    "Multiple linear regression is a statistical technique that uses multiple linear regression to model more\n",
    "complex relationships between two or more independent variables and one dependent variable. It is used when \n",
    "there are two or more x variables\n",
    "\n",
    "Example:\n",
    "    House price prediction ( as  there are the number of feature are present in the dataset like \n",
    "    ( number of room  ,  location , size of house ,haunted) are multiple independent feature build the \n",
    "    strong reationship to affect  the dependent  feature 'house_price'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87df116f-b895-4b13-97be-7285fc4e99c1",
   "metadata": {},
   "source": [
    "##  QUES NO 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7f3fd-bc02-4cf4-bfb0-392776aa886b",
   "metadata": {},
   "outputs": [],
   "source": [
    " Assumptions of linear regression include:\n",
    "\n",
    "Linearity:\n",
    "     The relationship between the dependent and independent variables is linear.\n",
    "        \n",
    "Independence:\n",
    "    The observations are independent of each other.\n",
    "    \n",
    "Homoscedasticity:\n",
    "    The variance of the errors is constant across all levels of the independent variables.\n",
    "    \n",
    "Normality:\n",
    "    The errors follow a normal distribution.\n",
    "    \n",
    "No multicollinearity: \n",
    "    The independent variables are not highly correlated with each other.\n",
    "No endogeneity: There is no relationship between the errors and the independent variables.\n",
    "\n",
    "\n",
    "There is a linear relationship between the predictors (x) and the outcome (y)\n",
    "\n",
    "Predictors (x) are independent and observed with negligible error.\n",
    "Residual Errors have a mean value of zero.\n",
    "Residual Errors have constant variance.\n",
    "Residual Errors are independent from each other and predictors (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72593a-f84f-4c55-84fc-1c9e33469532",
   "metadata": {},
   "source": [
    "## QUES NO 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5de923-9e6d-412f-9c25-c0bb408704de",
   "metadata": {},
   "outputs": [],
   "source": [
    "When modeling linear data, the slope and intercept of the graph provide useful information about the \n",
    "initial conditions and rate of change of what is being studied. First, the slope of a line is a measure\n",
    "of its steepness. In a line, slope is a ratio of the change in one variable to the change in the other. \n",
    "Usually, this refers to the change in y for each unit change in x, but sometimes other variables may be\n",
    "used.\n",
    "\n",
    "The intercept refers to the y-intercept, which is where the line intersects the y-axis. Again,\n",
    "other variables may be used, but the intercept generally refers to the independent variable and\n",
    "the vertical axis.\n",
    "\n",
    "To interpret the slope of the line, identify the variables in the situation. Since slope is change in y\n",
    "divided by change in x, divide the y-variable by the x-variable to get the units for the slope.\n",
    "\n",
    "Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a6632-14ad-49e3-82b5-c6309bbcaba5",
   "metadata": {},
   "source": [
    "## QUES NO 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8a1a1c-8acb-4403-a8f5-d4fdae678528",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient descent\n",
    "\n",
    "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models \n",
    "and neural networks.  Training data helps these models learn over time, and the cost function within \n",
    "gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter\n",
    "updates. Until the function is close to or equal to zero, the model will continue to adjust its parameters\n",
    "to yield the smallest possible error. Once machine learning models are optimized for accuracy, they can be\n",
    "powerful tools for artificial intelligence (AI) and computer science applications.\n",
    "\n",
    "In machine learning it is used for finding the global minima that's how the rate of error can be reduced \n",
    "\n",
    "Find the minimum of a function\n",
    "Update parameters in a model\n",
    "Minimize a cost function\n",
    "Optimize the parameters of models\n",
    "Improve the performance of models on a given task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d12b1-0e68-47a5-bf79-eac2088c398e",
   "metadata": {},
   "source": [
    "## QUES NO 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c4c620-1f57-4401-b0d8-57863e6e5072",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiple linear  regression\n",
    "\n",
    "Multiple linear regression is used to estimate the relationship between two or more independent variables\n",
    "and one dependent variable. You can use multiple linear regression when you want to know\n",
    "\n",
    "How strong the relationship is between two or more independent variables and one dependent variable\n",
    "(e.g. how rainfall, temperature, and amount of fertilizer added affect crop growth).\n",
    "The value of the dependent variable at a certain value of the independent variables (e.g. the expected \n",
    "yield of a crop at certain levels of rainfall, temperature, and fertilizer addition).\n",
    "\n",
    "Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions\n",
    "with multiple explanatory variables. Whereas linear regress only has one independent variable impacting\n",
    "the slope of the relationship, multiple regression incorporates multiple independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2f7173-e818-4a29-a0a6-17c2f96a5c3a",
   "metadata": {},
   "source": [
    "## QUES NO 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4023fc-7a80-427d-90d1-51071e286933",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multicollinearity: It generally occurs when the independent variables in a regression model are correlated\n",
    "with each other. This correlation is not expected as the independent variables are assumed to be independent. \n",
    "If the degree of this correlation is high, it may cause problems while predicting results from the model.\n",
    "\n",
    "Few Consequences of Multicollinearity\n",
    "\n",
    "The estimators have high variances and covariances which makes precise estimation difficult.\n",
    "Due to the above consequence in point 1, the confidence intervals tend to become wider which leads to the\n",
    "acceptance of the zero null hypothesis more often.\n",
    "The standard errors can be sensitive to small changes in the data.\n",
    "Coefficients become very sensitive to small changes in the model. It reduces the statistical power of \n",
    "the regression model.\n",
    "The effects of a single variable become difficult to distinguish from the other variables.\n",
    "\n",
    "Steps taken to remove the multicollinearity  problem in model\n",
    "\n",
    "Removing variables:\n",
    "    \n",
    "    A straightforward method of correcting multicollinearity is removing one or more\n",
    "variables showing a high correlation. This assists in reducing the multicollinearity linking correlated\n",
    "features. It is advisable to get rid of variables iteratively. We would begin with a variable with the\n",
    "highest VIF score since other variables are likely to capture its trend. As a result of removing this\n",
    "variable, other variables’ VIF values are likely to reduce.\n",
    "\n",
    "More data. Statistically, a regression model with more data is likely to suffer less variance due to a larger\n",
    "sample size. This will reduce the impact of multicollinearity:\n",
    "\n",
    "Using techniques such as partial least squares regression (PLS) and principal component analysis (PCA).\n",
    "A takeaway from this paper on partial least squares regression for multicollinearity is that PLS can lessen\n",
    "variables to a smaller grouping with no correlation between them. PLS, like PCA, is a dimensionality\n",
    "reduction technique. PCA reduces the dimension of data through the decomposition of data into independent \n",
    "\n",
    "factors. Therefore, new variables with no correlation between them are created. This article explains how\n",
    "PCA handles multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4373fc0-a8e8-4b9d-841b-4f68bac37374",
   "metadata": {},
   "source": [
    "## QUES NO 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09965d47-9e2e-4af5-b0d2-508e5782e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Polynomial regression is a form of Linear regression where only due to the Non-linear relationship between \n",
    "dependent and independent variables, we add some polynomial terms to linear regression to convert it into \n",
    "Polynomial regression.\n",
    "\n",
    "In polynomial regression, the relationship between the dependent variable and the independent variable is \n",
    "modeled as an nth-degree polynomial function. When the polynomial is of degree 2, it is called a quadratic\n",
    "model; when the degree of a polynomial is 3, it is called a cubic model, and so on.\n",
    "\n",
    "Suppose we have a dataset where variable X represents the Independent data and Y is the dependent data.\n",
    "Before feeding data to a mode in the preprocessing stage, we convert the input variables into polynomial\n",
    "terms using some degree.\n",
    "\n",
    "Consider an example my input value is 35, and the degree of a polynomial is 2, so I will find 35 power 0,\n",
    "35 power 1, and 35 power 2 this helps to interpret the non-linear relationship in data.\n",
    "The equation of polynomials becomes something like this.\n",
    "\n",
    "                 y = a0 + a1x1 + a2x12 + … + anx1n\n",
    "    \n",
    "Polynomial Regression is able to model non-linearly seperable data and is much more flexible than linear \n",
    "regression,  because linear regression can performed only when there is linear data is present "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61898bd5-9124-42f3-b017-465f24bf6f8f",
   "metadata": {},
   "source": [
    "## QUES NO 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42e30e-cb26-412f-b577-4c47ea638611",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantage of polynomial regression as compare to the linear regression \n",
    "\n",
    "1 : Polynomial regression are  can be applied when there is the non linear dataset is present in the dataset\n",
    "and it drawn the curve shape line as a best fit line \n",
    "2 : Polynomial provides the best approximation of the relationship between the dependent and independent\n",
    "variable.  Broad range of function can be fit under it. It basically fits a wide range of curvature\n",
    "\n",
    "Disadvantage of polynomial regression as compare to the linear regression \n",
    "\n",
    "1 : It is applied when there is the non-linear dataset is  present in the dataset\n",
    "\n",
    "I  will use the polynomial linear regression when there is the non  linear relation dataset is present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c039f4e-7392-42c3-bf39-310dfda428c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1295a1-ffa2-4589-9e8f-0e4884615f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
